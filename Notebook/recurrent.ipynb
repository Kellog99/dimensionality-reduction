{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2238e48-84bd-4729-9230-80bbdd4e0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import yaml\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ac8c7-a2fb-4005-a3aa-50f57a645968",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = config['dataset']['batch_size']\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Grayscale(),\n",
    "                                transforms.Normalize((0), (0.3))])\n",
    "\n",
    "config['model'][\"dataset\"] = data \n",
    "\n",
    "train_dataset = getattr(datasets, data)(root='./data', train=True, download=True, transform= transform)\n",
    "test_dataset = getattr(datasets, data)(root='./data', train=False, download=True, transform= transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "input_feat = len(train_dataset[0][0].flatten(0))\n",
    "\n",
    "criterium = nn.L1Loss(reduction = \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc2e57-e290-44ba-8931-fb492e46d7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59503aa6-eaab-4a74-93ee-8c0269c2fd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2e758-d2f4-4d86-8d10-eee948c2ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(loss_train: list, \n",
    "         loss_val: list, \n",
    "         model, \n",
    "         ds, \n",
    "         config: yaml, \n",
    "         transform: None, \n",
    "         pieces_of_loss_train:dict = None,\n",
    "         pieces_of_loss_val:dict = None):\n",
    "    if type(ds.targets) == list:\n",
    "        targets = torch.tensor(ds.targets).unique()\n",
    "    else:\n",
    "        targets = ds.targets.unique()\n",
    "    n_images = np.min([config['plot']['n_images'], len(targets)])\n",
    "    targets = targets[:n_images]\n",
    "    dataset = config['model']['dataset']             \n",
    "    show_pieces = config['plot']['show_pieces']\n",
    "    show_rec = config['plot']['show_rec']\n",
    "    show_training = config['plot']['show_training']\n",
    "    model_type = {\n",
    "        1: \"auto\",\n",
    "        2: \"vae\",\n",
    "        3: \"vae_recurrent\"\n",
    "    }\n",
    "    model_type = model_type[config['model']['model']]\n",
    "        \n",
    "    l = {'epoch' :range(1, len(loss_train)+1),\n",
    "         'training':loss_train, \n",
    "         'validation':loss_val}\n",
    "    fig = px.line(l, \n",
    "                  x ='epoch', \n",
    "                  y=['training','validation'],\n",
    "                  title = \"Loss of the training\",\n",
    "                  width = 700, \n",
    "                  height = 600)\n",
    "    if show_training:\n",
    "        fig.show()\n",
    "        \n",
    "\n",
    "    y = list(pieces_of_loss_train.keys())\n",
    "    y.remove('epoch')\n",
    "    fig = px.line(pieces_of_loss_train, \n",
    "                  x ='epoch', \n",
    "                  y= y,\n",
    "                  title = \"pieces of the training loss\",\n",
    "                  width = 800, \n",
    "                  height = 700)\n",
    "    \n",
    "    fig.write_html(os.path.join(config['paths']['images'],model_type, f'piece_train_{dataset}.html'))\n",
    "    if show_pieces:\n",
    "        fig.show()\n",
    "\n",
    "    fig = px.line(pieces_of_loss_val, \n",
    "                  x = 'epoch', \n",
    "                  y= y,\n",
    "                  title = \"pieces of the validation loss\",\n",
    "                  width = 800, \n",
    "                  height = 700)\n",
    "    \n",
    "    fig.write_html(os.path.join(config['paths']['images'],model_type, f'piece_val_{dataset}.html'))\n",
    "    if show_pieces:\n",
    "        fig.show()\n",
    "             \n",
    "    #reconstruction part\n",
    "\n",
    "    fig, axes = plt.subplots(nrows = n_images, \n",
    "                             ncols = 2, \n",
    "                             figsize = (6, n_images*3),\n",
    "                             constrained_layout=True)\n",
    "    title = {\n",
    "        1: \"Autoencoder\",\n",
    "        2: \"Variational autoencoder\",\n",
    "        3: \"Vae Recurrent\"\n",
    "    }\n",
    "    title = title[config['model']['model']]\n",
    "\n",
    "    fig.suptitle(f\"{title} {model.hidden_dim}D for {dataset}\")\n",
    "    with torch.no_grad():\n",
    "        for i, target in enumerate(targets):\n",
    "            if type(ds.targets) == list:\n",
    "                data = ds.data[[True if x == i else False for x in ds.targets]][0]\n",
    "            else:\n",
    "                data = ds.data[ds.targets==target][0].numpy()\n",
    "\n",
    "            data = transform(data)[0]\n",
    "            data = data.unsqueeze(0) if len(data.shape) == 2 else data\n",
    "\n",
    "            if config['model']['model'] in [2,3]: \n",
    "                recon, _, _, _,_ = model(data.float().to(model.device))\n",
    "            else:\n",
    "                recon = model.cpu()(data.float().flatten(1))\n",
    "            axes[i,0].imshow(data[0], cmap = 'gray')\n",
    "            axes[i,1].imshow(recon.detach().cpu().numpy().reshape(data.shape[1:]), cmap = 'gray')\n",
    "        \n",
    "            axes[i,0].set_title(\"real\")\n",
    "            axes[i,1].set_title(\"reconstructed\")\n",
    "    plt.savefig(os.path.join(config['paths']['images'],model_type, f'rec_{model.hidden_dim}D_{dataset}.png'))    \n",
    "    if show_rec:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95733fd-6a8e-4c64-8972-d52a76cc80d4",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5aa41e-ee0a-478d-b1c5-1ffcec4caa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dist_fun_rec(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inverse: bool, \n",
    "                 input_feat:int, \n",
    "                 hidden_dim:int = 16):\n",
    "        super(dist_fun_rec, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inverse = inverse\n",
    "        self.input_feat = input_feat\n",
    "\n",
    "        # Encoder layers        \n",
    "        fc1 = [nn.Linear(input_feat, hidden_dim), \n",
    "               nn.Sigmoid(), \n",
    "               nn.Linear(hidden_dim, 1)]\n",
    "\n",
    "        self.fc1 = nn.Sequential(*fc1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n",
    "        \n",
    "    def derivative(self, x):\n",
    "        derivative = vmap(jacrev(self.forward))(x)\n",
    "        return derivative\n",
    "\n",
    "    def functional_loss(self, conditional, var = None):\n",
    "        batch = conditional.shape[0]\n",
    "        device = conditional.device.type\n",
    "        \n",
    "        if self.inverse:\n",
    "            # La funzione inversa deve produrre campioni di media zero e varianza 1\n",
    "            # Genero 500 rv in [0,1]\n",
    "            u = torch.rand(batch, 500, 1, requires_grad = True).float().to(device)\n",
    "            u = torch.cat((u, conditional.unsqueeze(1).repeat(1,500,1)),-1)          # B x 500 x (c+1)\n",
    "            X = self.forward(u)\n",
    "    \n",
    "            ### Voglio che mu = 0 e std = 1\n",
    "            mean = torch.mean(X)\n",
    "            std = torch.mean(X**2)-mean**2\n",
    "\n",
    "            zero = torch.mean(torch.abs(mean))\n",
    "            one = torch.mean(torch.abs(std - torch.ones_like(std)))\n",
    "            l = zero + one\n",
    "            return l\n",
    "        else:    \n",
    "            #### proprietà densità\n",
    "            # 1) lim_{x --> -infty} F(x)=0\n",
    "            # 2) lim_{x --> infty} F(x)=1\n",
    "            x = -40 * torch.ones(batch, 1 , requires_grad = True).float().to(device)\n",
    "            lw = torch.cat((conditional, x), -1 )\n",
    "            up = torch.cat((conditional, -x), -1)\n",
    "            lower = self.forward(lw)\n",
    "            upper = self.forward(up)\n",
    "            \n",
    "            zero = torch.mean(torch.abs(lower))\n",
    "            one = torch.mean(torch.abs(upper-torch.ones_like(upper)))\n",
    "            \n",
    "            # 3) F è crescente ==> controllo in un dominio [a,b]\n",
    "            a = -30\n",
    "            b = 30\n",
    "            domain = torch.rand(batch, 500, 1, device = device)*(b-a) + a\n",
    "            input_pos = torch.cat((domain, conditional.unsqueeze(1).repeat(1,500,1)),-1).requires_grad_()\n",
    "            density = torch.cat([self.derivative(input_pos[i])[:,:,0].view(1,-1) for i in range(x.shape[0])])\n",
    "            positivity = torch.sum(F.relu(-density))\n",
    "\n",
    "            # Poichè la derivata è una densità allora il suo integrale deve essere 1\n",
    "            prob = torch.sum(density, -1)\n",
    "            normality = torch.mean(torch.abs(prob-torch.ones_like(prob)))   \n",
    "            l = zero + one + positivity + normality\n",
    "        return l\n",
    "        \n",
    "\n",
    "\n",
    "class VAE_recurrent(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_feat: int,\n",
    "                 criterium,\n",
    "                 device, \n",
    "                 hidden_dim: int):\n",
    "        super(VAE_recurrent, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.criterium = criterium\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.input_feat = input_feat\n",
    "        self.upper_bound_var = torch.tensor([5.]*hidden_dim, device = device, requires_grad = True).float()\n",
    "        self.fc1 = nn.Sequential(nn.Flatten(1),\n",
    "                                 nn.Linear(input_feat, 512),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(512, 256))\n",
    "        \n",
    "        self.fc_mu = nn.Sequential(nn.Linear(256, 128),\n",
    "                                   nn.Tanh(),\n",
    "                                   nn.Linear(128, hidden_dim))\n",
    "        \n",
    "        self.fc_logvar = nn.Sequential(nn.Linear(256, 128),\n",
    "                                       nn.Tanh(),\n",
    "                                       nn.Linear(128, hidden_dim))\n",
    "\n",
    "        # Decoder layers\n",
    "        \n",
    "        self.fc2 = nn.Sequential(nn.Tanh(), \n",
    "                                 nn.Linear(hidden_dim, 128),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(128, 256),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(256, 512),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(512, input_feat))\n",
    "\n",
    "        #### F^{-1}(u) ####\n",
    "        self.F_inv = nn.ModuleList([dist_fun_rec(inverse = True, input_feat = i+1) for i in range(hidden_dim)])\n",
    "\n",
    "        #### F(F^{-1}(u)) ####\n",
    "        self.F = nn.ModuleList([dist_fun_rec(inverse = False, input_feat = i+1) for i in range(hidden_dim)])\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_logvar(x)\n",
    "        log_var = torch.max(torch.min(log_var,torch.ones_like(log_var)*4),torch.ones_like(log_var)*(-4)) \n",
    "        var = torch.exp(log_var)\n",
    "        return mu, var#.view(-1,self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.fc2(z)\n",
    "\n",
    "        \n",
    "    def reparameterize(self, mu, var):\n",
    "\n",
    "        #### Generating the random distribution #####\n",
    "        b,_ = mu.shape\n",
    "        eps = torch.tensor([]).to(self.device)\n",
    "        input = []\n",
    "        decode = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            u = torch.rand(b,1, requires_grad = True).float().to(self.device)\n",
    "            input.append(u)\n",
    "            x = self.F_inv[i](torch.cat((u, eps),-1))\n",
    "            eps = torch.cat((x, eps), -1)\n",
    "            decode.append(self.F[i](eps))\n",
    "            \n",
    "        # tutti gli input che sono stati generati dalla distribuzione uniforme\n",
    "        u = torch.cat(input, -1)\n",
    "        \n",
    "        # tutte le ricostruzioni generate dalla rete\n",
    "        u_hat = torch.cat(decode, -1)\n",
    "        ### Perturbing the embedding \n",
    "        z = mu + var*eps\n",
    "        return z, u, u_hat, eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, var = self.encode(x.view(-1, self.input_feat))\n",
    "        z, u, u_hat, eps, = self.reparameterize(mu, var)\n",
    "        x_reconstructed = self.decode(z)\n",
    "        \n",
    "        return x_reconstructed, u, eps, u_hat, var\n",
    "    \n",
    "     \n",
    "    def loss_density(self, u, eps, var):\n",
    "        loss_density_F = 0\n",
    "        loss_density_F_inv = 0\n",
    "        loss_derivative = 0\n",
    "\n",
    "        ### Kullenback Leiberg divergence        \n",
    "        kl = torch.tensor([0]).float().to(self.device)\n",
    "                \n",
    "        for i in range(self.hidden_dim):\n",
    "            dFinv_du = self.F[i].derivative(torch.cat((u[:,i:i+1],eps[:,:i]), -1))[:, 0, 0]\n",
    "            dF_dy = self.F[i].derivative(eps[:,:i+1])[:, 0, 0]\n",
    "            \n",
    "            loss_derivative +=  self.criterium(dFinv_du, dF_dy)\n",
    "            loss_density_F += self.F[i].functional_loss(eps[:,:i])\n",
    "            loss_density_F_inv += self.F_inv[i].functional_loss(eps[:,:i])\n",
    "        \n",
    "            if len(dF_dy[dF_dy>0])>0:\n",
    "                kl += torch.mean(torch.log(var[:,i][dF_dy>0]) - torch.log(var[:,i][dF_dy>0]))\n",
    "\n",
    "        l = loss_density_F + loss_density_F_inv + loss_derivative + kl\n",
    "        return l, (loss_derivative.item(), loss_density_F.item(), loss_density_F_inv.item(), kl.item())\n",
    "    \n",
    "    def loss_functional(self, img, img_rec, u, eps, u_hat, var):\n",
    "        \n",
    "        \n",
    "        ### reconstruction loss for distribution\n",
    "        reconstruction1 =  self.criterium(u, u_hat)\n",
    "        ### reconstruction loss for image\n",
    "        reconstruction2 = self.criterium(img, img_rec)\n",
    "        ### Anti annullamento varianza\n",
    "        var_emb = torch.mean(torch.prod(var, 1))\n",
    "    \n",
    "        l = reconstruction1 + 500*reconstruction2 + 1/var_emb\n",
    "        \n",
    "        return l, (reconstruction1.item(), reconstruction2.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301017b-74df-4fed-9adf-fe376478d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(model, \n",
    "         dataloader,\n",
    "         optimizer,\n",
    "         pieces_of_loss: dict, \n",
    "         training: bool = False):\n",
    "    loss_epoch = 0.0\n",
    "    len_load = len(dataloader)\n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    for data, _ in tqdm(iter(dataloader)):\n",
    "        # blocking the gradient summation \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # forward step\n",
    "        x_reconstructed, u, x, u_hat, var  = model(data.to(model.device).float())\n",
    "        \n",
    "        # computing the loss\n",
    "        l1, dens = model.loss_density(u, x, var)\n",
    "        l2, func = model.loss_functional(data.to(model.device).flatten(1).float(), \n",
    "                                         x_reconstructed, u, x, u_hat, var)\n",
    "        loss = l1 + l2 \n",
    "        \n",
    "        pieces = dens + func\n",
    "        if torch.any(torch.isnan(loss)).item():\n",
    "            print(dens)\n",
    "            print(func)\n",
    "        # Backward and optimize\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        loss_epoch += loss.item()\n",
    "        for i, key in enumerate(pieces_of_loss.keys()):\n",
    "            if 'std_emb' == key:\n",
    "                pieces_of_loss[key][-1] += torch.mean(torch.prod(var,1)).item()/len_load\n",
    "            else:\n",
    "                pieces_of_loss[key][-1] += pieces[i]/len_load\n",
    "    return loss_epoch/len_load\n",
    "\n",
    "def train_recurrent(model, \n",
    "             train_loader, \n",
    "             val_loader, \n",
    "             num_epochs,\n",
    "             optimizer):\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    be = np.inf\n",
    "    bm = model\n",
    "    \n",
    "    pieces_of_loss_train = {'loss_derivative':[], 'loss_density_F':[], 'loss_density_F_inv':[], 'kl_loss':[],\n",
    "                            'reconstruction1':[], 'reconstruction2':[], 'std_emb':[]}    \n",
    "    pieces_of_loss_val = {'loss_derivative':[], 'loss_density_F':[], 'loss_density_F_inv':[], 'kl_loss':[],\n",
    "                            'reconstruction1':[], 'reconstruction2':[],  'std_emb':[]} \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):    \n",
    "        for key in pieces_of_loss_train.keys():\n",
    "            pieces_of_loss_train[key].append(0)\n",
    "            pieces_of_loss_val[key].append(0)\n",
    "        l = step(model, train_loader, optimizer, pieces_of_loss_train, True)\n",
    "        loss_train.append(l)\n",
    "        l = step(model, val_loader, optimizer,pieces_of_loss_val)\n",
    "        loss_val.append(l)\n",
    "        if (epoch+1)%5==0:\n",
    "            print(f\"loss training at the {epoch+1}-th = {loss_train[-1]}\")\n",
    "            print(f\"loss validation at the {epoch+1}-th = {loss_val[-1]}\")\n",
    "            \n",
    "        if loss_val[-1]<be:\n",
    "            be = loss_val[-1]\n",
    "            bm = model\n",
    "    pieces_of_loss_train['epoch'] = list(range(1, num_epochs+1))\n",
    "    pieces_of_loss_val['epoch'] = list(range(1, num_epochs+1))\n",
    "    return bm, loss_train, loss_val, pieces_of_loss_train, pieces_of_loss_val\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
